{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import bz2\n",
    "import io\n",
    "from bz2 import BZ2File\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style(\"white\")\n",
    "sns.set(rc = {'figure.figsize':(12,9)})\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None  # default='warn', Mutes warnings when copying a slice from a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charger le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>journal</th>\n",
       "      <th>date</th>\n",
       "      <th>page</th>\n",
       "      <th>ppage</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>JDG</td>\n",
       "      <td>1978-12-29</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algérie : l'arbitrage sans arbitre Pour un Eta...</td>\n",
       "      <td>3777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>JDG</td>\n",
       "      <td>1978-12-29</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALORS QUE LA FOULE ENTERRE SES DERNIERES VICTI...</td>\n",
       "      <td>3401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100</td>\n",
       "      <td>JDG</td>\n",
       "      <td>1978-06-13</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>La Ruhr chinoise s'ouvre à l'Occident Gilbert ...</td>\n",
       "      <td>6017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>10000</td>\n",
       "      <td>JDG</td>\n",
       "      <td>1978-10-25</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Selon le président du directoire de la Banque ...</td>\n",
       "      <td>4345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>100000</td>\n",
       "      <td>GDL</td>\n",
       "      <td>1976-10-05</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TÉLÉVISION-RADIO SELECTION-TV Antenne 2 20.30 ...</td>\n",
       "      <td>7976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id journal       date  page  ppage  \\\n",
       "0            0     JDG 1978-12-29     1    NaN   \n",
       "1            1     JDG 1978-12-29     3    NaN   \n",
       "100        100     JDG 1978-06-13     1    NaN   \n",
       "10000    10000     JDG 1978-10-25    11    NaN   \n",
       "100000  100000     GDL 1976-10-05     2    NaN   \n",
       "\n",
       "                                                     text  length  \n",
       "0       Algérie : l'arbitrage sans arbitre Pour un Eta...    3777  \n",
       "1       ALORS QUE LA FOULE ENTERRE SES DERNIERES VICTI...    3401  \n",
       "100     La Ruhr chinoise s'ouvre à l'Occident Gilbert ...    6017  \n",
       "10000   Selon le président du directoire de la Banque ...    4345  \n",
       "100000  TÉLÉVISION-RADIO SELECTION-TV Antenne 2 20.30 ...    7976  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned = pd.read_json('cleaned.json.bz2', compression = 'bz2')\n",
    "df_cleaned['journal'] = df_cleaned['journal'].astype('category')\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.drop(columns = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.index = np.arange(len(df_cleaned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage ciblé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_count(df, keywords):\n",
    "    \"\"\" Select text that contains certain keywords and count the latter \n",
    "    \n",
    "    Inputs:\n",
    "        df(pandas DataFrame): contains all articles in the 'text' column\n",
    "        keywords(list<str>): list of keywords to search\n",
    "        \n",
    "    Output:\n",
    "        counts(list<list<int>>): count of all keywords in all articles\n",
    "    \"\"\"\n",
    "    \n",
    "    counts, k = [], []\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        k.append(keyword.lower())\n",
    "    \n",
    "    for ind, row in df.iterrows():\n",
    "        \n",
    "        counts_ = []\n",
    "        for k_ in k:\n",
    "            counts_.append(len(re.findall(k_, row['text'].lower())))\n",
    "        \n",
    "        counts.append(counts_)\n",
    "                \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "garbage = ['tableau hebdomadaire',\n",
    "           'aetoa',\n",
    "           'sopacx',\n",
    "           'cotées en suisse',\n",
    "           'new york stock exch',\n",
    "           'new york nys',\n",
    "           'volvo',\n",
    "           'nyse',\n",
    "           'nestlé',\n",
    "           'heineken',\n",
    "           'ericsson',\n",
    "           'stock exch',\n",
    "           'siemens',\n",
    "           'yorkst',\n",
    "           'quebec hydro',\n",
    "           'aetn a',\n",
    "           'raytheo',\n",
    "           'euro devises',\n",
    "           'royal dutch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_garbage = keywords_count(df_cleaned, garbage)\n",
    "counts_garbage = np.asarray(counts_garbage).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(garbage)):\n",
    "    df_cleaned['keyword_' + garbage[i]] = counts_garbage[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(garbage)):\n",
    "    df_cleaned = df_cleaned[df_cleaned['keyword_' + garbage[i]] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440317"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isoler le corpus pour la dernière couche de nettoyage ciblée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['pétrol', \n",
    "            'opep', \n",
    "            'organisation des pays exportateurs de pétrole',\n",
    "            'shell',\n",
    "            'tamoil',\n",
    "            'petrol',\n",
    "            'petroleum',\n",
    "            'texaco',\n",
    "            'standard oil',\n",
    "            'exxon',\n",
    "            'gulf oil',\n",
    "            'ats',\n",
    "            'afp',\n",
    "            'reuter',\n",
    "            'reuther']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = keywords_count(df_cleaned, keywords)\n",
    "counts = np.asarray(counts).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(keywords)):\n",
    "    df_cleaned['keyword_' + keywords[i]] = counts[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['keywords_petrole'] = df_cleaned['keyword_pétrol'] + df_cleaned['keyword_petrol']\n",
    "df_cleaned['keywords_OPEP'] = df_cleaned['keyword_opep'] + \\\n",
    "    df_cleaned['keyword_organisation des pays exportateurs de pétrole']\n",
    "df_cleaned['keywords_compagnies'] = df_cleaned['keyword_shell'] + \\\n",
    "    df_cleaned['keyword_tamoil'] + df_cleaned['keyword_petroleum'] + df_cleaned['keyword_texaco'] + \\\n",
    "    df_cleaned['keyword_standard oil'] + df_cleaned['keyword_exxon'] + df_cleaned['keyword_gulf oil']\n",
    "df_cleaned['keywords'] = df_cleaned['keywords_petrole'] + df_cleaned['keywords_OPEP'] + \\\n",
    "    df_cleaned['keywords_compagnies']\n",
    "\n",
    "df_cleaned['depeches'] = df_cleaned['keyword_afp'] + df_cleaned['keyword_ats'] + \\\n",
    "df_cleaned['keyword_reuther'] + df_cleaned['keyword_reuter']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus = df_cleaned[df_cleaned['keywords'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage post-filtrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depeches = df_corpus[df_corpus['depeches'] > 0]\n",
    "df_not_depeches = df_corpus[df_corpus['depeches'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17853"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_depeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_texts(txt1, txt2):\n",
    "    ''' Compare texts to check for duplicates. Proceeds in two steps (to spare ressources). First\n",
    "    by aligning roughly the two texts and then by verifiying more precisely this alignment.\n",
    "    \n",
    "    Inputs:\n",
    "        txt1(str): First text to compare\n",
    "        txt2(str): Second text to compare\n",
    "        \n",
    "    Output:\n",
    "        (bool): returns True if the texts are duplicates, else False\n",
    "    '''\n",
    "    \n",
    "    identical = False\n",
    "    recover = 0\n",
    "        \n",
    "    ### First step is a rough assessment ###\n",
    "    \n",
    "    text1 = txt1.split(' ')\n",
    "    text2 = txt2.split(' ')\n",
    "    \n",
    "    if (len(text1) > 20) and (len(text2) > 20):\n",
    "        text1, text2 = text1[:20], text2[:20]\n",
    "        \n",
    "    else:\n",
    "        min_ = min(len(text1), len(text2))\n",
    "        text1, text2 = text1[:min_], text2[:min_]\n",
    "        \n",
    "    if set(text1) == set(text2):\n",
    "        return True\n",
    "    \n",
    "    differentials, grade = [], 0\n",
    "    \n",
    "    # estimate identity \n",
    "    for i in range(len(text1)):\n",
    "        for j in range(len(text2)):\n",
    "            if text1[i] == text2[j]:\n",
    "                grade += 1\n",
    "                differentials.append(j - i)\n",
    "    \n",
    "    _ = np.unique(differentials, return_counts = True)\n",
    "    \n",
    "    if len(_[1]) == 0:\n",
    "        return False\n",
    "    \n",
    "    else:\n",
    "            \n",
    "        ### Second step is a finer assessment ###\n",
    "        if (grade >= 12) and (np.argmax(_[1].tolist()) >= 12): \n",
    "            \n",
    "            if (len(text1) > 50) and (len(text2) > 50):\n",
    "                text1, text2 = text1[:50], text2[:50]\n",
    "                \n",
    "                differentials, grade = [], 0\n",
    "                \n",
    "                for i in range(len(text1)):\n",
    "                    for j in range(len(text2)):\n",
    "                        if text1[i] == text2[j]:\n",
    "                            grade += 1\n",
    "                            differentials.append(j - i)\n",
    "                            \n",
    "                _ = np.unique(differentials, return_counts = True)\n",
    "                \n",
    "                if (grade >= 40) and (np.argmax(_[1].tolist()) >= 40):\n",
    "                    return True\n",
    "\n",
    "            else:\n",
    "                return True\n",
    "    \n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(df):\n",
    "    ''' Search for duplicates in all articles'''\n",
    "\n",
    "    first_day = min(df_depeches['date'])\n",
    "    last_day = max(df_depeches['date'])\n",
    "    day = first_day\n",
    "    \n",
    "    duplicates, checked = [], []\n",
    "\n",
    "    while day <= last_day:\n",
    "        border = [day - timedelta(days = 7), day + timedelta(days = 7)]\n",
    "        df_1 = df[df['date'] == day] # ?\n",
    "        df_2 = df[df['date'] >= border[0]]\n",
    "        df_2 = df_2[df_2['date'] <= border[1]]\n",
    "        \n",
    "        for ind_1, row_1 in df_1.iterrows():\n",
    "            for ind_2, row_2 in df_2.iterrows():\n",
    "                \n",
    "                if not(ind_2 in checked):\n",
    "                    if (ind_1 != ind_2) and compare_texts(row_1['text'], row_2['text']):\n",
    "                        duplicates.append([ind_1, ind_2])\n",
    "            \n",
    "            checked.append(ind_1)\n",
    "        \n",
    "        day += timedelta(days = 1)\n",
    "        \n",
    "    return duplicates\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ats = df_corpus[df_corpus['keyword_ats'] > 0]\n",
    "df_reuter = df_corpus[df_corpus['keyword_reuter'] > 0]\n",
    "df_reuther = df_corpus[df_corpus['keyword_reuther'] > 0]\n",
    "df_afp = df_corpus[df_corpus['keyword_afp'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "duplicates = check_duplicates(df_reuther)\n",
    "duplicates += check_duplicates(df_reuter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates += check_duplicates(df_afp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates += check_duplicates(df_ats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = []\n",
    "\n",
    "for i in range(len(duplicates)):\n",
    "    if not(duplicates[i][0] in to_drop) and not(duplicates[i][1] in to_drop):\n",
    "        to_drop.append(duplicates[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1410"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus_unique = df_corpus.drop(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22549"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21139"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_corpus_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus_unique.to_json('corpus_unique.json.bz2', compression = 'bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus_unique = pd.read_json('corpus_unique.json.bz2', compression = 'bz2')\n",
    "df_corpus_unique['journal'] = df_corpus_unique['journal'].astype('category')\n",
    "df_corpus_unique.index = np.arange(len(df_corpus_unique))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df_corpus_unique.drop(columns = ['keyword_tableau hebdomadaire', 'keyword_aetoa', 'keyword_sopacx',\n",
    "                                      'keyword_cotées en suisse', 'keyword_new york stock exch',\n",
    "                                      'keyword_new york nys'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création des sous-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['pétrol', \n",
    "            'opep', \n",
    "            'organisation des pays exportateurs de pétrole',\n",
    "            'shell',\n",
    "            'tamoil',\n",
    "            'petrol',\n",
    "            'petroleum',\n",
    "            'texaco',\n",
    "            'standard oil',\n",
    "            'exxon',\n",
    "            'gulf oil',\n",
    "            'ats',\n",
    "            'afp',\n",
    "            'reuter',\n",
    "            'reuther']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = keywords_count(df_, keywords)\n",
    "counts = np.asarray(counts).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(keywords)):\n",
    "    df_['keyword_' + keywords[i]] = counts[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_['keywords_petrole'] = df_['keyword_pétrol'] + df_['keyword_petrol']\n",
    "df_['keywords_OPEP'] = df_['keyword_opep'] + \\\n",
    "    df_['keyword_organisation des pays exportateurs de pétrole']\n",
    "df_['keywords_compagnies'] = df_['keyword_shell'] + \\\n",
    "    df_['keyword_tamoil'] + df_['keyword_petroleum'] + df_['keyword_texaco'] + \\\n",
    "    df_['keyword_standard oil'] + df_['keyword_exxon'] + df_['keyword_gulf oil']\n",
    "df_['keywords'] = df_['keywords_petrole'] + df_['keywords_OPEP'] + \\\n",
    "    df_['keywords_compagnies']\n",
    "\n",
    "df_['depeches'] = df_['keyword_afp'] + df_['keyword_ats'] + df_['keyword_reuther'] + df_['keyword_reuter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus = df_[df_['keywords'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde des sous-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus_OPEP = df_corpus[df_corpus['keywords_OPEP'] > 0]\n",
    "df_corpus_petrole = df_corpus[df_corpus['keywords_petrole'] > 0]\n",
    "df_corpus_compagnies = df_corpus[df_corpus['keywords_compagnies'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depeches = df_corpus[df_corpus['depeches'] > 0]\n",
    "df_not_depeches = df_corpus[df_corpus['depeches'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus.to_json('corpus.json.bz2', compression = 'bz2')\n",
    "df_corpus_OPEP.to_json('corpus_OPEP.json.bz2', compression = 'bz2')\n",
    "df_corpus_petrole.to_json('corpus_petrole.json.bz2', compression = 'bz2')\n",
    "df_corpus_compagnies.to_json('corpus_compagnies.json.bz2', compression = 'bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_json('total.json.bz2', compression = 'bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depeches.to_json('corpus_depeches.json.bz2', compression = 'bz2')\n",
    "df_not_depeches.to_json('corpus_not_depeches.json.bz2', compression = 'bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus = pd.read_json('corpus.json.bz2', compression = 'bz2')\n",
    "df_corpus_OPEP = pd.read_json('corpus_OPEP.json.bz2', compression = 'bz2')\n",
    "df_corpus_petrole = pd.read_json('corpus_petrole.json.bz2', compression = 'bz2')\n",
    "df_corpus_compagnies = pd.read_json('corpus_compagnies.json.bz2', compression = 'bz2')\n",
    "df_depeches = pd.read_json('corpus_depeches.json.bz2', compression = 'bz2')\n",
    "df_not_depeches = pd.read_json('corpus_not_depeches.json.bz2', compression = 'bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.read_json('total.json.bz2', compression = 'bz2')\n",
    "df_total['journal'] = df_total['journal'].astype('category')\n",
    "df_total.index = np.arange(len(df_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus['journal'] = df_corpus['journal'].astype('category')\n",
    "df_corpus_OPEP['journal'] = df_corpus_OPEP['journal'].astype('category')\n",
    "df_corpus_petrole['journal'] = df_corpus_petrole['journal'].astype('category')\n",
    "df_corpus_compagnies['journal'] = df_corpus_compagnies['journal'].astype('category')\n",
    "\n",
    "df_corpus.index = np.arange(len(df_corpus))\n",
    "df_corpus_OPEP.index = np.arange(len(df_corpus_OPEP))\n",
    "df_corpus_petrole.index = np.arange(len(df_corpus_petrole))\n",
    "df_corpus_compagnies.index = np.arange(len(df_corpus_compagnies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export au format Iramuteq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal</th>\n",
       "      <th>date</th>\n",
       "      <th>page</th>\n",
       "      <th>ppage</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>keyword_volvo</th>\n",
       "      <th>keyword_nyse</th>\n",
       "      <th>keyword_nestlé</th>\n",
       "      <th>keyword_heineken</th>\n",
       "      <th>...</th>\n",
       "      <th>keyword_gulf oil</th>\n",
       "      <th>keyword_ats</th>\n",
       "      <th>keyword_afp</th>\n",
       "      <th>keyword_reuter</th>\n",
       "      <th>keyword_reuther</th>\n",
       "      <th>keywords_petrole</th>\n",
       "      <th>keywords_OPEP</th>\n",
       "      <th>keywords_compagnies</th>\n",
       "      <th>keywords</th>\n",
       "      <th>depeches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JDG</td>\n",
       "      <td>1974-01-07</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pétrole : \" (déjà 50000 la crise continue chôm...</td>\n",
       "      <td>3465</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JDG</td>\n",
       "      <td>1971-03-03</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A l'est de Suez La Grande-Bretagne quittera le...</td>\n",
       "      <td>3335</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JDG</td>\n",
       "      <td>1971-09-23</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Après une séance unique de trois heures Fin de...</td>\n",
       "      <td>1275</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JDG</td>\n",
       "      <td>1971-07-10</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>. Conférence extraordinaire des. pays exportat...</td>\n",
       "      <td>2560</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JDG</td>\n",
       "      <td>1971-01-22</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Les compagnies pétrclières créent un bureau pe...</td>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  journal       date  page  ppage  \\\n",
       "0     JDG 1974-01-07     2    NaN   \n",
       "1     JDG 1971-03-03    16    NaN   \n",
       "2     JDG 1971-09-23     5    NaN   \n",
       "3     JDG 1971-07-10     5    NaN   \n",
       "4     JDG 1971-01-22     5    NaN   \n",
       "\n",
       "                                                text  length  keyword_volvo  \\\n",
       "0  Pétrole : \" (déjà 50000 la crise continue chôm...    3465              0   \n",
       "1  A l'est de Suez La Grande-Bretagne quittera le...    3335              0   \n",
       "2  Après une séance unique de trois heures Fin de...    1275              0   \n",
       "3  . Conférence extraordinaire des. pays exportat...    2560              0   \n",
       "4  Les compagnies pétrclières créent un bureau pe...     913              0   \n",
       "\n",
       "   keyword_nyse  keyword_nestlé  keyword_heineken    ...     keyword_gulf oil  \\\n",
       "0             0               0                 0    ...                    0   \n",
       "1             0               0                 0    ...                    0   \n",
       "2             0               0                 0    ...                    0   \n",
       "3             0               0                 0    ...                    0   \n",
       "4             0               0                 0    ...                    0   \n",
       "\n",
       "   keyword_ats  keyword_afp  keyword_reuter  keyword_reuther  \\\n",
       "0            2            1               0                0   \n",
       "1            4            1               0                0   \n",
       "2            0            1               0                0   \n",
       "3            1            3               0                0   \n",
       "4            0            1               0                0   \n",
       "\n",
       "   keywords_petrole  keywords_OPEP  keywords_compagnies  keywords  depeches  \n",
       "0                 6              5                    0        11         3  \n",
       "1                 0              1                    0         1         5  \n",
       "2                 2              4                    0         6         1  \n",
       "3                15              9                    0        24         4  \n",
       "4                 2              1                    0         3         1  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus_OPEP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formate_text(df):\n",
    "    \n",
    "    text = ''\n",
    "    for row in df.iterrows():\n",
    "        text += '**** *' + row[1][0] + ' *' + \\\n",
    "            str(row[1][1])[:10] + ' *' + str(row[1][2]) + ' *' + \\\n",
    "            str(row[1][5]) + '\\n' + row[1][4] + '\\n'\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iramuteq_export(df, filename):\n",
    "    \n",
    "    text = formate_text(df)\n",
    "    file = open(filename,'w') \n",
    "    file.write(text)\n",
    "    file.close()\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iramuteq_export(df_corpus, 'corpus.txt')\n",
    "iramuteq_export(df_corpus_OPEP, 'corpus_opep.txt')\n",
    "iramuteq_export(df_corpus_petrole, 'corpus_petrole.txt')\n",
    "iramuteq_export(df_corpus_compagnies, 'corpus_compagnies.txt')\n",
    "iramuteq_export(df_depeches, 'depeches.txt')\n",
    "iramuteq_export(df_not_depeches, 'not_depeches.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
